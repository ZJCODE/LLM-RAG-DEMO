{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Load & Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "- Load: First we need to load our data. \n",
    "\n",
    "- Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "\n",
    "- Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\n",
    "\n",
    "- Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "\n",
    "- Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pymupdf\n",
    "# !pip install pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from typing import Generator\n",
    "import pymupdf4llm\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocLoader:\n",
    "    \"\"\"\n",
    "    A class to load and process documents from a specified directory.\n",
    "    \n",
    "    Attributes:\n",
    "        path (str): The path to the directory containing the documents.\n",
    "        chunk_size (int): The size of the text chunks to split the documents into.\n",
    "        chunk_overlap (int): The amount of overlap between text chunks.\n",
    "        enable_logging (bool): Flag to show progress of loading documents.\n",
    "        docs (Generator): A generator yielding processed markdown documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path: str,chunk_size=512,chunk_overlap=128,enable_logging=True):\n",
    "        \"\"\"\n",
    "        Initializes the DocLoader with the specified path and loads the documents.\n",
    "        \n",
    "        Args:\n",
    "            path (str): The path to the directory containing the documents.\n",
    "            chunk_size (int): The size of the text chunks to split the documents into.\n",
    "            chunk_overlap (int): The amount of overlap between text chunks.\n",
    "            enable_logging (bool): Flag to show progress of loading documents.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.enable_logging = enable_logging\n",
    "        if self.enable_logging:\n",
    "            print(\"\\nLoading files from {}\".format(self.path))\n",
    "        self.docs = self.load()\n",
    "        self.file_count = self.count_total_markdown_files()\n",
    "\n",
    "    def load(self) -> Generator:\n",
    "        \"\"\"\n",
    "        Loads the documents by processing PDFs and loading markdown files.\n",
    "        \n",
    "        Returns:\n",
    "            Generator: A generator yielding processed markdown documents.\n",
    "        \"\"\"\n",
    "        self.process_pdfs()\n",
    "        return self.load_markdown_files()\n",
    "\n",
    "    def process_pdfs(self):\n",
    "        \"\"\"\n",
    "        Converts PDF files in the specified directory and all subdirectories to markdown format.\n",
    "        \"\"\"\n",
    "        if self.enable_logging:\n",
    "            print(\"\\nProcessing PDFs\")\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".pdf\"):\n",
    "                    base_name = os.path.splitext(file)[0]\n",
    "                    md_file_path = os.path.join(root, f\"{base_name}_pdf_converted.md\")\n",
    "                    if not os.path.exists(md_file_path):\n",
    "                        if self.enable_logging:\n",
    "                            print(f\"Converted {os.path.join(root, file)} to markdown\")\n",
    "                        md_text = pymupdf4llm.to_markdown(os.path.join(root, file))\n",
    "                        pathlib.Path(md_file_path).write_bytes(md_text.encode())\n",
    "                    else:\n",
    "                        if self.enable_logging:\n",
    "                            print(f\"Skipping {os.path.join(root, file)} as it has already been converted to markdown\")\n",
    "\n",
    "    def count_total_markdown_files(self) -> int:\n",
    "        \"\"\"\n",
    "        Counts the total number of markdown files in the specified directory and all subdirectories.\n",
    "        \n",
    "        Returns:\n",
    "            int: The total number of markdown files.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".md\"):\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    def load_markdown_files(self) -> Generator:\n",
    "        \"\"\"\n",
    "        Loads markdown files from the specified directory and all subdirectories.\n",
    "        \n",
    "        Yields:\n",
    "            tuple: A tuple containing header splits and text chunks of the markdown file.\n",
    "        \"\"\"\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".md\"):\n",
    "                    yield self.load_markdown(os.path.join(root, file), self.chunk_size, self.chunk_overlap)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_markdown(file: str, chunks=512, chunk_overlap=128) -> tuple:\n",
    "        \"\"\"\n",
    "        Loads and splits a markdown file into headers and text chunks.\n",
    "        \n",
    "        Args:\n",
    "            file (str): The path to the markdown file.\n",
    "            chunks (int): The size of the text chunks to split the documents into.\n",
    "            chunk_overlap (int): The amount of overlap between text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: A tuple containing the file name, text chunks, and metadata.\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            md_text = f.read()\n",
    "\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            (\"##\", \"Header 2\"),\n",
    "            (\"###\", \"Header 3\"),\n",
    "            (\"####\", \"Header 4\"),\n",
    "            (\"#####\", \"Header 5\"),\n",
    "        ]\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "        md_header_splits = markdown_splitter.split_text(md_text)\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunks, chunk_overlap=chunk_overlap)\n",
    "        chunks = text_splitter.split_documents(md_header_splits)\n",
    "        \n",
    "        documents = [chunk.page_content for chunk in chunks]\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata.update({\"file\": file})\n",
    "        metadatas = [chunk.metadata for chunk in chunks]\n",
    "        \n",
    "        return file,documents,metadatas\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_single_file(file: str, chunks=512, chunk_overlap=128) -> tuple:\n",
    "        # file can be a pdf or a markdown file\n",
    "        # if file is a pdf, convert it to markdown then call load_markdown\n",
    "        if file.endswith(\".pdf\"):\n",
    "            base_name = os.path.splitext(file)[0]\n",
    "            md_file_path = f\"{base_name}_pdf_converted.md\"\n",
    "            if not os.path.exists(md_file_path):\n",
    "                md_text = pymupdf4llm.to_markdown(file)\n",
    "                pathlib.Path(md_file_path).write_bytes(md_text.encode())\n",
    "            return DocLoader.load_markdown(md_file_path, chunks, chunk_overlap)\n",
    "        elif file.endswith(\".md\"):\n",
    "            return DocLoader.load_markdown(file, chunks, chunk_overlap)\n",
    "        else:\n",
    "            raise ValueError(\"File must be a PDF or markdown file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading files from doc\n",
      "\n",
      "Processing PDFs\n",
      "Skipping doc/example_doc.pdf as it has already been converted to markdown\n",
      "Skipping doc/subdir/subdir_example_doc.pdf as it has already been converted to markdown\n",
      "doc/example_doc_pdf_converted.md\n",
      "doc/example_doc.md\n",
      "doc/subdir/subdir_example_doc.md\n",
      "doc/subdir/subdir_example_doc_pdf_converted.md\n"
     ]
    }
   ],
   "source": [
    "dl = DocLoader(\"doc\")\n",
    "for doc in dl.docs:\n",
    "    print(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading files from doc\n",
      "\n",
      "Processing PDFs\n",
      "Skipping doc/example_doc.pdf as it has already been converted to markdown\n",
      "Skipping doc/subdir/subdir_example_doc.pdf as it has already been converted to markdown\n"
     ]
    }
   ],
   "source": [
    "dl = DocLoader(\"doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current file: doc/example_doc_pdf_converted.md\n"
     ]
    }
   ],
   "source": [
    "file,documents,metadatas = next(dl.docs)\n",
    "print(\"current file:\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['**Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs)**  \\n**1. Introduction**  \\nRetrieval-Augmented Generation (RAG) is a powerful approach that combines the strengths of\\ninformation retrieval and generative models. By integrating a retrieval mechanism with a large  \\nlanguage model (LLM), RAG can provide more accurate and contextually relevant responses, especially\\nin knowledge-intensive tasks.  \\n**2. Objectives**  \\nEnhance the accuracy of responses generated by LLMs.',\n",
       " 'in knowledge-intensive tasks.  \\n**2. Objectives**  \\nEnhance the accuracy of responses generated by LLMs.  \\nProvide up-to-date information by retrieving relevant documents from a knowledge base.  \\nImprove user experience by delivering contextually rich and informative answers.  \\n**3. Components**  \\n3.1 Large Language Model (LLM)  \\nAn LLM, such as GPT-4, is responsible for generating human-like text based on input prompts. It can',\n",
       " 'An LLM, such as GPT-4, is responsible for generating human-like text based on input prompts. It can  \\nunderstand context, generate coherent responses, and adapt to various conversational styles.  \\n3.2 Document Retrieval System  \\nThis system is responsible for fetching relevant documents or snippets from a predefined knowledge\\nbase. It can utilize techniques such as:  \\n**Keyword Search: Simple matching of keywords in the query to documents.**',\n",
       " 'base. It can utilize techniques such as:  \\n**Keyword Search: Simple matching of keywords in the query to documents.**  \\n**Semantic Search: Using embeddings to find documents that are semantically similar to the query.**  \\n3.3 Knowledge Base  \\nA structured repository of documents, articles, or any relevant information that the retrieval system can  \\naccess. This can include:  \\nFAQs  \\nTechnical documentation  \\nResearch papers  \\nWeb pages  \\n-----  \\n1. User Query: The user inputs a question or prompt.',\n",
       " 'FAQs  \\nTechnical documentation  \\nResearch papers  \\nWeb pages  \\n-----  \\n1. User Query: The user inputs a question or prompt.  \\n2. Document Retrieval:  \\nThe query is processed to identify relevant documents from the knowledge base.  \\nThe retrieval system returns a set of top-N documents based on relevance.  \\n3. Response Generation:  \\nThe LLM takes the user query and the retrieved documents as input.  \\nIt generates a response that incorporates information from the retrieved documents.',\n",
       " 'It generates a response that incorporates information from the retrieved documents.  \\n4. Output: The generated response is presented to the user.  \\n**5. Implementation Steps**  \\n5.1 Setting Up the Knowledge Base  \\nCollect and curate relevant documents.  \\nIndex the documents for efficient retrieval.  \\n5.2 Building the Retrieval System  \\nChoose a retrieval method (e.g., Elasticsearch, FAISS).  \\nImplement the retrieval logic to fetch documents based on user queries.  \\n5.3 Integrating the LLM',\n",
       " \"Implement the retrieval logic to fetch documents based on user queries.  \\n5.3 Integrating the LLM  \\nSet up the LLM environment (e.g., using Hugging Face Transformers).  \\nCreate a function to generate responses using the LLM and the retrieved documents.  \\n5.4 Testing and Evaluation  \\nConduct tests with various queries to evaluate the system's performance.  \\nMeasure accuracy, relevance, and user satisfaction.  \\n**6. Use Cases**\",\n",
       " 'Measure accuracy, relevance, and user satisfaction.  \\n**6. Use Cases**  \\n**Customer Support: Providing accurate answers to customer inquiries by retrieving relevant**  \\nsupport documents.  \\n**Research Assistance: Helping researchers find and summarize relevant literature.**  \\n**Content Creation: Assisting writers by generating content based on retrieved information.**  \\n-----  \\n**Retrieval Accuracy: Improving the relevance of retrieved documents to enhance response quality.**',\n",
       " '-----  \\n**Retrieval Accuracy: Improving the relevance of retrieved documents to enhance response quality.**  \\n**LLM Limitations: Addressing the inherent limitations of LLMs, such as generating incorrect or**\\nbiased information.  \\n**8. Conclusion**  \\nRAG with LLMs presents a promising approach to enhance the capabilities of conversational agents  \\nand information retrieval systems. By effectively combining retrieval and generation, we can create  \\nmore intelligent and responsive applications.  \\n-----']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'},\n",
       " {'file': 'doc/example_doc_pdf_converted.md'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('doc/example_doc_pdf_converted.md',\n",
       " ['**Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs)**  \\n**1. Introduction**  \\nRetrieval-Augmented Generation (RAG) is a powerful approach that combines the strengths of\\ninformation retrieval and generative models. By integrating a retrieval mechanism with a large  \\nlanguage model (LLM), RAG can provide more accurate and contextually relevant responses, especially\\nin knowledge-intensive tasks.  \\n**2. Objectives**  \\nEnhance the accuracy of responses generated by LLMs.',\n",
       "  'in knowledge-intensive tasks.  \\n**2. Objectives**  \\nEnhance the accuracy of responses generated by LLMs.  \\nProvide up-to-date information by retrieving relevant documents from a knowledge base.  \\nImprove user experience by delivering contextually rich and informative answers.  \\n**3. Components**  \\n3.1 Large Language Model (LLM)  \\nAn LLM, such as GPT-4, is responsible for generating human-like text based on input prompts. It can',\n",
       "  'An LLM, such as GPT-4, is responsible for generating human-like text based on input prompts. It can  \\nunderstand context, generate coherent responses, and adapt to various conversational styles.  \\n3.2 Document Retrieval System  \\nThis system is responsible for fetching relevant documents or snippets from a predefined knowledge\\nbase. It can utilize techniques such as:  \\n**Keyword Search: Simple matching of keywords in the query to documents.**',\n",
       "  'base. It can utilize techniques such as:  \\n**Keyword Search: Simple matching of keywords in the query to documents.**  \\n**Semantic Search: Using embeddings to find documents that are semantically similar to the query.**  \\n3.3 Knowledge Base  \\nA structured repository of documents, articles, or any relevant information that the retrieval system can  \\naccess. This can include:  \\nFAQs  \\nTechnical documentation  \\nResearch papers  \\nWeb pages  \\n-----  \\n1. User Query: The user inputs a question or prompt.',\n",
       "  'FAQs  \\nTechnical documentation  \\nResearch papers  \\nWeb pages  \\n-----  \\n1. User Query: The user inputs a question or prompt.  \\n2. Document Retrieval:  \\nThe query is processed to identify relevant documents from the knowledge base.  \\nThe retrieval system returns a set of top-N documents based on relevance.  \\n3. Response Generation:  \\nThe LLM takes the user query and the retrieved documents as input.  \\nIt generates a response that incorporates information from the retrieved documents.',\n",
       "  'It generates a response that incorporates information from the retrieved documents.  \\n4. Output: The generated response is presented to the user.  \\n**5. Implementation Steps**  \\n5.1 Setting Up the Knowledge Base  \\nCollect and curate relevant documents.  \\nIndex the documents for efficient retrieval.  \\n5.2 Building the Retrieval System  \\nChoose a retrieval method (e.g., Elasticsearch, FAISS).  \\nImplement the retrieval logic to fetch documents based on user queries.  \\n5.3 Integrating the LLM',\n",
       "  \"Implement the retrieval logic to fetch documents based on user queries.  \\n5.3 Integrating the LLM  \\nSet up the LLM environment (e.g., using Hugging Face Transformers).  \\nCreate a function to generate responses using the LLM and the retrieved documents.  \\n5.4 Testing and Evaluation  \\nConduct tests with various queries to evaluate the system's performance.  \\nMeasure accuracy, relevance, and user satisfaction.  \\n**6. Use Cases**\",\n",
       "  'Measure accuracy, relevance, and user satisfaction.  \\n**6. Use Cases**  \\n**Customer Support: Providing accurate answers to customer inquiries by retrieving relevant**  \\nsupport documents.  \\n**Research Assistance: Helping researchers find and summarize relevant literature.**  \\n**Content Creation: Assisting writers by generating content based on retrieved information.**  \\n-----  \\n**Retrieval Accuracy: Improving the relevance of retrieved documents to enhance response quality.**',\n",
       "  '-----  \\n**Retrieval Accuracy: Improving the relevance of retrieved documents to enhance response quality.**  \\n**LLM Limitations: Addressing the inherent limitations of LLMs, such as generating incorrect or**\\nbiased information.  \\n**8. Conclusion**  \\nRAG with LLMs presents a promising approach to enhance the capabilities of conversational agents  \\nand information retrieval systems. By effectively combining retrieval and generation, we can create  \\nmore intelligent and responsive applications.  \\n-----'],\n",
       " [{'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'},\n",
       "  {'file': 'doc/example_doc_pdf_converted.md'}])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.load_single_file(\"doc/example_doc.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
